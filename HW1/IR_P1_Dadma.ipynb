{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuaCFMBqXJgo"
      },
      "source": [
        "# Download and Extract Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddpftRxHdTvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531966a6-49b4-454c-e95f-960d989cbd7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Useful-Corpora-for-Text-Mining-in-Persian-Language'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 18 (from 1)\u001b[K\n",
            "Receiving objects: 100% (18/18), 478.78 MiB | 24.46 MiB/s, done.\n",
            "Updating files: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "![ -d \"Useful-Corpora-for-Text-Mining-in-Persian-Language\" ] || git clone \"https://github.com/Text-Mining/Useful-Corpora-for-Text-Mining-in-Persian-Language\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SIb8DCGdvws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a864451-80ef-49e8-b585-faaa861b514f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/Useful-Corpora-for-Text-Mining-in-Persian-Language/News/FarsNews 97/farsnews.part01.rar\n",
            "\n",
            "Extracting  farsnews.json                                                \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\n",
            "\n",
            "Extracting from /content/Useful-Corpora-for-Text-Mining-in-Persian-Language/News/FarsNews 97/farsnews.part02.rar\n",
            "\n",
            "...         farsnews.json                                                \b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\n",
            "\n",
            "Extracting from /content/Useful-Corpora-for-Text-Mining-in-Persian-Language/News/FarsNews 97/farsnews.part03.rar\n",
            "\n",
            "...         farsnews.json                                                \b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b100%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ],
      "source": [
        "!pip install -q unrar\n",
        "![ -f farsnews.json ] || unrar x '/content/Useful-Corpora-for-Text-Mining-in-Persian-Language/News/FarsNews 97/farsnews.part01.rar'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXco0-14XThC"
      },
      "source": [
        "#Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izc5QQuBXTAR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "counter = 0\n",
        "news=[] # { CategoryPanel[], NewsDate, NewsTitle, NewsSummary, NewsBody, GetComments: { CommentsJsonArray[] } }\n",
        "\n",
        "# Read just one line of data for now\n",
        "for line in open('farsnews.json','r', encoding='utf-8-sig'):\n",
        "  news.append(json.loads(line))\n",
        "  counter+=1\n",
        "  if(counter == 1):\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dadma Module\n"
      ],
      "metadata": {
        "id": "1tdkuLd7OI8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modules Installation"
      ],
      "metadata": {
        "id": "6OlxrB1eLuWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH_O6X5s2Djm"
      },
      "outputs": [],
      "source": [
        "# dadmatools installation\n",
        "!pip install dadmatools\n",
        "# Docs: https://github.com/Dadmatech/DadmaTools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "okXE_YYkK7zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dadmatools.normalizer import Normalizer as DadmaNormalizer\n",
        "\n",
        "# Pipeline\n",
        "# Containing Tokenizer, Lemmatizer, POS Tagger, Dependancy Parser, Constituency Parser, Kasreh, spellcheker.\n",
        "import dadmatools.pipeline.language as language"
      ],
      "metadata": {
        "id": "iSRS3PKbosAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XrHHzxo4Djc"
      },
      "source": [
        "## Process Data With Dadma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_list(lists):\n",
        "  r = []\n",
        "  for x in lists:\n",
        "    r.append(x['text'])\n",
        "  return r"
      ],
      "metadata": {
        "id": "EoO4Q_5cji3w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ncb4GP435A6U"
      },
      "outputs": [],
      "source": [
        "# Clean and standardize the text\n",
        "def dadma_normalize(text):\n",
        "  # preprocesing => remove '\\n'\n",
        "  removeNewline = text.replace(\"\\r\\n\", \"\")\n",
        "\n",
        "  normalizer = DadmaNormalizer(\n",
        "    # full_cleaning=True,\n",
        "    # unify_chars=True,\n",
        "    # refine_punc_spacing=True,\n",
        "    remove_extra_space=True,\n",
        "    remove_puncs=True,\n",
        "    # remove_html=False,\n",
        "    # remove_stop_word=False,\n",
        "    # replace_email_with=\"<EMAIL>\",\n",
        "    # replace_number_with=None,\n",
        "    # replace_url_with=\"\",\n",
        "    # replace_mobile_number_with=None,\n",
        "    # replace_emoji_with=None,\n",
        "    # replace_home_number_with=None\n",
        "  )\n",
        "  normalized = normalizer.normalize(removeNewline)\n",
        "\n",
        "  print(\"(DADMA-TOOLS): normalized body: \", normalized)\n",
        "  return normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UqqkiFCA5Evv"
      },
      "outputs": [],
      "source": [
        "# Splits the text into individual words or sentences\n",
        "def dadma_tokenize(text):\n",
        "\n",
        "  # here lemmatizer and pos tagger will be loaded\n",
        "  # as tokenizer is the default tool, it will be loaded as well even without calling\n",
        "  # pips = 'tok,lem,pos,dep,chunk,cons,spellchecker,kasreh'\n",
        "\n",
        "  pips = 'tok' #we need tokenizer\n",
        "  nlp = language.Pipeline(pips)\n",
        "\n",
        "  # you can see the pipeline with this code\n",
        "  # print(nlp.analyze_pipes(pretty=True))\n",
        "\n",
        "  # tokenized_text is an SpaCy object\n",
        "  tokenized_text = nlp(text)\n",
        "  token_list = to_list(language.to_json(pips, tokenized_text)[0])\n",
        "\n",
        "  print(\"(DADMA-TOOLS): tokenized body: \", token_list)\n",
        "  return token_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce words to their root or stem form\n",
        "def dadma_stem(text):\n",
        "  # pips = 'tok,lem,pos,dep,chunk,cons,spellchecker,kasreh'\n",
        "  pips = 'tok,lem'\n",
        "  nlp = language.Pipeline(pips)\n",
        "\n",
        "  results = language.to_json(pips, nlp(text))\n",
        "  stemmed = []\n",
        "  for word in results[0]:\n",
        "    stemmed.append(word['lemma'])\n",
        "\n",
        "  print(\"(DADMA-TOOLS): stemmer result: \", stemmed)"
      ],
      "metadata": {
        "id": "O-kU-JJyI1g_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gpRRpXsD4-FJ"
      },
      "outputs": [],
      "source": [
        "# Take a news article as input and apply the steps to its body text\n",
        "def dadma_process_news(news):\n",
        "  news_body = news['NewsBody']\n",
        "  normalized_body = dadma_normalize(news_body)\n",
        "  tokenized_body = dadma_tokenize(normalized_body)\n",
        "  stemmed_body = dadma_stem(normalized_body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNvin7Z5cupa"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxDqhtvJ7jtE"
      },
      "outputs": [],
      "source": [
        "for i in range(counter):\n",
        "  print(f\"Text: {news[i]['NewsBody']}\\n\")\n",
        "\n",
        "  print(\"dadmatools Proccess: \")\n",
        "  dadma_process_news(news[i])"
      ]
    }
  ]
}